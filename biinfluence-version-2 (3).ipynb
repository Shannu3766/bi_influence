{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/working/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:29:47.863588Z","iopub.execute_input":"2025-11-06T15:29:47.863858Z","iopub.status.idle":"2025-11-06T15:29:48.115049Z","shell.execute_reply.started":"2025-11-06T15:29:47.863837Z","shell.execute_reply":"2025-11-06T15:29:48.114470Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# !pip install git+https://github.com/Shannu3766/bi_influence.git\n\n!pip install --upgrade --no-cache-dir git+https://github.com/Shannu3766/bi_influence.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:29:48.116326Z","iopub.execute_input":"2025-11-06T15:29:48.116682Z","iopub.status.idle":"2025-11-06T15:30:51.978575Z","shell.execute_reply.started":"2025-11-06T15:29:48.116657Z","shell.execute_reply":"2025-11-06T15:30:51.977862Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/Shannu3766/bi_influence.git\n  Cloning https://github.com/Shannu3766/bi_influence.git to /tmp/pip-req-build-x0od99c6\n  Running command git clone --filter=blob:none --quiet https://github.com/Shannu3766/bi_influence.git /tmp/pip-req-build-x0od99c6\n  Resolved https://github.com/Shannu3766/bi_influence.git to commit d5b497115e41852e6b8a1409751ee5bf39c8fc7b\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from adaptive_lora==2.1.0) (2.6.0+cu124)\nRequirement already satisfied: transformers>=4.30 in /usr/local/lib/python3.11/dist-packages (from adaptive_lora==2.1.0) (4.53.3)\nRequirement already satisfied: peft>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from adaptive_lora==2.1.0) (0.16.0)\nRequirement already satisfied: datasets>=2.0 in /usr/local/lib/python3.11/dist-packages (from adaptive_lora==2.1.0) (4.1.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from adaptive_lora==2.1.0) (4.67.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from adaptive_lora==2.1.0) (1.26.4)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from adaptive_lora==2.1.0) (1.2.2)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from adaptive_lora==2.1.0) (1.9.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from adaptive_lora==2.1.0) (2.2.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0->adaptive_lora==2.1.0) (3.19.1)\nCollecting pyarrow>=21.0.0 (from datasets>=2.0->adaptive_lora==2.1.0)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0->adaptive_lora==2.1.0) (0.4.0)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0->adaptive_lora==2.1.0) (2.32.5)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0->adaptive_lora==2.1.0) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0->adaptive_lora==2.1.0) (0.70.16)\nRequirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0->adaptive_lora==2.1.0) (2025.9.0)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0->adaptive_lora==2.1.0) (1.0.0rc2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0->adaptive_lora==2.1.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0->adaptive_lora==2.1.0) (6.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->adaptive_lora==2.1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->adaptive_lora==2.1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->adaptive_lora==2.1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->adaptive_lora==2.1.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->adaptive_lora==2.1.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->adaptive_lora==2.1.0) (2.4.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft>=0.3.0->adaptive_lora==2.1.0) (7.1.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft>=0.3.0->adaptive_lora==2.1.0) (0.5.3)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->adaptive_lora==2.1.0) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->adaptive_lora==2.1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->adaptive_lora==2.1.0) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13->adaptive_lora==2.1.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13->adaptive_lora==2.1.0)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13->adaptive_lora==2.1.0)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13->adaptive_lora==2.1.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13->adaptive_lora==2.1.0)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13->adaptive_lora==2.1.0)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13->adaptive_lora==2.1.0)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13->adaptive_lora==2.1.0)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13->adaptive_lora==2.1.0)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->adaptive_lora==2.1.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->adaptive_lora==2.1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->adaptive_lora==2.1.0) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13->adaptive_lora==2.1.0)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->adaptive_lora==2.1.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->adaptive_lora==2.1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->adaptive_lora==2.1.0) (1.3.0)\nCollecting huggingface-hub>=0.24.0 (from datasets>=2.0->adaptive_lora==2.1.0)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30->adaptive_lora==2.1.0) (2025.9.18)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30->adaptive_lora==2.1.0) (0.21.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->adaptive_lora==2.1.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->adaptive_lora==2.1.0) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->adaptive_lora==2.1.0) (2025.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->adaptive_lora==2.1.0) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->adaptive_lora==2.1.0) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->adaptive_lora==2.1.0) (3.6.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0->adaptive_lora==2.1.0) (3.12.15)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.0->adaptive_lora==2.1.0) (1.1.10)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->adaptive_lora==2.1.0) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.0->adaptive_lora==2.1.0) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.0->adaptive_lora==2.1.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.0->adaptive_lora==2.1.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.0->adaptive_lora==2.1.0) (2025.8.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13->adaptive_lora==2.1.0) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->adaptive_lora==2.1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->adaptive_lora==2.1.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->adaptive_lora==2.1.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->adaptive_lora==2.1.0) (2024.2.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0->adaptive_lora==2.1.0) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0->adaptive_lora==2.1.0) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0->adaptive_lora==2.1.0) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0->adaptive_lora==2.1.0) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0->adaptive_lora==2.1.0) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0->adaptive_lora==2.1.0) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0->adaptive_lora==2.1.0) (1.20.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->adaptive_lora==2.1.0) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m201.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m249.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m240.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m299.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m192.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m289.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m260.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m259.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m248.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m189.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m330.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: adaptive_lora\n  Building wheel for adaptive_lora (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for adaptive_lora: filename=adaptive_lora-2.1.0-py3-none-any.whl size=7345 sha256=da1599efb3ac163543165856ee5eb4a644e03f187e4d355322fd28320dd54428\n  Stored in directory: /tmp/pip-ephem-wheel-cache-sbr06x_2/wheels/4b/91/1d/e153ff3aa3759d6ccdcc4706d8559f1cbfaa4aa22b992958cd\nSuccessfully built adaptive_lora\nInstalling collected packages: pyarrow, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, nvidia-cusolver-cu12, adaptive_lora\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed adaptive_lora-2.1.0 huggingface-hub-0.36.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyarrow-22.0.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q datasets evaluate accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:30:51.979580Z","iopub.execute_input":"2025-11-06T15:30:51.979824Z","iopub.status.idle":"2025-11-06T15:30:55.596333Z","shell.execute_reply.started":"2025-11-06T15:30:51.979803Z","shell.execute_reply":"2025-11-06T15:30:55.595422Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -U datasets==2.20.0 pyarrow==15.0.2 transformers==4.44.2 evaluate==0.4.2 --no-cache-dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:30:55.598512Z","iopub.execute_input":"2025-11-06T15:30:55.598812Z","iopub.status.idle":"2025-11-06T15:31:11.128477Z","shell.execute_reply.started":"2025-11-06T15:30:55.598789Z","shell.execute_reply":"2025-11-06T15:31:11.127798Z"}},"outputs":[{"name":"stdout","text":"Collecting datasets==2.20.0\n  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\nCollecting pyarrow==15.0.2\n  Downloading pyarrow-15.0.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\nCollecting transformers==4.44.2\n  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting evaluate==0.4.2\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (3.19.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (1.26.4)\nCollecting pyarrow-hotfix (from datasets==2.20.0)\n  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\nCollecting dill<0.3.9,>=0.3.0 (from datasets==2.20.0)\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (2.32.5)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (0.70.16)\nCollecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0)\n  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (3.12.15)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (2025.9.18)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (0.5.3)\nCollecting tokenizers<0.20,>=0.19 (from transformers==4.44.2)\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (1.20.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==2.20.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==2.20.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==2.20.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==2.20.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==2.20.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets==2.20.0) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2025.8.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets==2.20.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets==2.20.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets==2.20.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets==2.20.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets==2.20.0) (2024.2.0)\nDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-15.0.2-cp311-cp311-manylinux_2_28_x86_64.whl (38.3 MB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m255.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m239.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m297.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m300.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m335.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m257.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\nInstalling collected packages: pyarrow-hotfix, fsspec, dill, tokenizers, pyarrow, datasets, transformers, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.9.0\n    Uninstalling fsspec-2025.9.0:\n      Successfully uninstalled fsspec-2025.9.0\n  Attempting uninstall: dill\n    Found existing installation: dill 0.4.0\n    Uninstalling dill-0.4.0:\n      Successfully uninstalled dill-0.4.0\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 22.0.0\n    Uninstalling pyarrow-22.0.0:\n      Successfully uninstalled pyarrow-22.0.0\n  Attempting uninstall: datasets\n    Found existing installation: datasets 4.1.1\n    Uninstalling datasets-4.1.1:\n      Successfully uninstalled datasets-4.1.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n  Attempting uninstall: evaluate\n    Found existing installation: evaluate 0.4.6\n    Uninstalling evaluate-0.4.6:\n      Successfully uninstalled evaluate-0.4.6\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 evaluate-0.4.2 fsspec-2024.5.0 pyarrow-15.0.2 pyarrow-hotfix-0.7 tokenizers-0.19.1 transformers-4.44.2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# !pip uninstall adaptive_lora -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:31:11.129375Z","iopub.execute_input":"2025-11-06T15:31:11.129645Z","iopub.status.idle":"2025-11-06T15:31:11.133479Z","shell.execute_reply.started":"2025-11-06T15:31:11.129622Z","shell.execute_reply":"2025-11-06T15:31:11.132763Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# !pip install git+https://github.com/Shannu3766/bi_influence.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:31:11.134518Z","iopub.execute_input":"2025-11-06T15:31:11.134700Z","iopub.status.idle":"2025-11-06T15:31:11.145488Z","shell.execute_reply.started":"2025-11-06T15:31:11.134685Z","shell.execute_reply":"2025-11-06T15:31:11.144861Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ============================================================\n# 泅 Step 1: Install Dependencies and Package\n# ============================================================\n!pip install -q datasets evaluate accelerate scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:31:11.146231Z","iopub.execute_input":"2025-11-06T15:31:11.146386Z","iopub.status.idle":"2025-11-06T15:31:14.569548Z","shell.execute_reply.started":"2025-11-06T15:31:11.146373Z","shell.execute_reply":"2025-11-06T15:31:14.568722Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ============================================================\n# 泅 STEP 1: Install dependencies & your Adaptive LoRA package\n# ============================================================\n!pip install -q datasets evaluate accelerate scikit-learn pandas matplotlib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:31:14.570895Z","iopub.execute_input":"2025-11-06T15:31:14.571283Z","iopub.status.idle":"2025-11-06T15:31:18.060773Z","shell.execute_reply.started":"2025-11-06T15:31:14.571259Z","shell.execute_reply":"2025-11-06T15:31:18.059792Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    Trainer, \n    TrainingArguments\n)\nfrom datasets import load_dataset, Dataset\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom torch.utils.data import DataLoader\nfrom adaptive_lora.callbacks import AdaptiveLoRACallback # Import our callback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:31:18.061864Z","iopub.execute_input":"2025-11-06T15:31:18.062187Z","iopub.status.idle":"2025-11-06T15:31:37.151981Z","shell.execute_reply.started":"2025-11-06T15:31:18.062153Z","shell.execute_reply":"2025-11-06T15:31:37.151354Z"}},"outputs":[{"name":"stderr","text":"2025-11-06 15:31:24.270221: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762443084.462665      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762443084.520844      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.Tensor size changed, may indicate binary incompatibility. Expected 64 from C header, got 80 from PyObject\n<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.ChunkedArray size changed, may indicate binary incompatibility. Expected 64 from C header, got 72 from PyObject\n<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib._Tabular size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject\n<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.Table size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# --- 1. Setup Model and Tokenizer ---","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:31:37.154091Z","iopub.execute_input":"2025-11-06T15:31:37.154646Z","iopub.status.idle":"2025-11-06T15:31:37.158326Z","shell.execute_reply.started":"2025-11-06T15:31:37.154627Z","shell.execute_reply":"2025-11-06T15:31:37.157507Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"model_name = \"mistralai/Mistral-7B-v0.1\" # Example model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, \n    torch_dtype=torch.bfloat16, # Use bfloat16 for efficiency\n    device_map=\"auto\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:31:37.158982Z","iopub.execute_input":"2025-11-06T15:31:37.159234Z","iopub.status.idle":"2025-11-06T15:34:00.849137Z","shell.execute_reply.started":"2025-11-06T15:31:37.159214Z","shell.execute_reply":"2025-11-06T15:34:00.848572Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1827c2d472874304b7df540703fb4867"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94b8032fe9db49cb8d17cf3f89915dc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e58322bdfd8d4c1d931cc61e2845ab70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d959e30eb5a48a5be3278871b88703d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52443b2ddfb945c6880c6bdae779d939"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70bb28800de244379ab4de28da53ade7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9925a39dad648f88fec6d88bd042027"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06c8ebbd50a446a0b1ca4243f4ef35ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86b8f17bac224383a195d530a89700b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a9c68ab93ec4e0cab79fbea60afddea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8b68e70436b4555ad2c6f9b525f8637"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    task_type=TaskType.CAUSAL_LM,\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:34:00.849836Z","iopub.execute_input":"2025-11-06T15:34:00.850061Z","iopub.status.idle":"2025-11-06T15:34:04.206316Z","shell.execute_reply.started":"2025-11-06T15:34:00.850026Z","shell.execute_reply":"2025-11-06T15:34:04.205478Z"}},"outputs":[{"name":"stdout","text":"trainable params: 13,631,488 || all params: 7,255,363,584 || trainable%: 0.1879\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 3. Prepare Datasets ---\n# (Using a dummy dataset for this example)\ndef dummy_dataset(tokenizer, n_samples=200): \n    text = \"The quick brown fox jumps over the lazy dog. \" * 50\n    data = [text] * n_samples\n    tokenized = tokenizer(data, truncation=True, padding=\"max_length\", max_length=128)\n    tokenized[\"labels\"] = tokenized[\"input_ids\"]\n    \n    # Convert the dictionary to a Dataset object\n    return Dataset.from_dict(tokenized)\n\ntrain_dataset = dummy_dataset(tokenizer, n_samples=25)\nval_dataset = dummy_dataset(tokenizer, n_samples=20) # Small val set","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:34:04.207190Z","iopub.execute_input":"2025-11-06T15:34:04.207469Z","iopub.status.idle":"2025-11-06T15:34:04.281769Z","shell.execute_reply.started":"2025-11-06T15:34:04.207442Z","shell.execute_reply":"2025-11-06T15:34:04.281242Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# --- 4. Create the val_dataloader for the callback ---\n# This dataloader provides the small batch for BI score computation\ndef collate_fn(batch):\n    # Convert list of dicts to dict of lists\n    features = {}\n    for key in batch[0].keys():\n        features[key] = [example[key] for example in batch]\n        \n    # Use tokenizer.pad to handle padding\n    padded_batch = tokenizer.pad(\n        features, \n        return_tensors=\"pt\",\n        padding=\"max_length\", # Ensure padding to max_length\n        max_length=128\n    )\n    return padded_batch\n\nval_loader_for_callback = DataLoader(\n    val_dataset, \n    batch_size=4, # Small batch size is sufficient\n    collate_fn=collate_fn\n)\n\n\nadaptive_callback = AdaptiveLoRACallback(\n    total_rank=128,\n    tau=0.5,\n    val_dataloader=val_loader_for_callback,\n    # min_rank=4 has been REMOVED to match Algorithm 2\n    verbose=True\n)\n\n# --- 6. Setup Trainer ---\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    logging_steps=10,\n    report_to=\"none\", # Disable wandb/etc. for this example\n    save_strategy=\"no\", # Don't save checkpoints\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset, \n    callbacks=[adaptive_callback], # Add our callback here\n)\n\n\nprint(\"Starting training with Adaptive LoRA...\")\ntrainer.train()\n\nprint(\"Training complete. Check logs/adaptive_lora_epoch_logs.csv for rank evolution.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:34:04.282440Z","iopub.execute_input":"2025-11-06T15:34:04.282615Z","iopub.status.idle":"2025-11-06T15:36:24.243057Z","shell.execute_reply.started":"2025-11-06T15:34:04.282602Z","shell.execute_reply":"2025-11-06T15:36:24.242407Z"}},"outputs":[{"name":"stdout","text":"Starting training with Adaptive LoRA...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [21/21 02:12, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.127000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.099300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","output_type":"stream"},{"name":"stdout","text":"\n--- AdaptiveLoRA: Starting rank update for Epoch 1 ---\nComputing BI importance scores...\nAllocating new ranks...\nApplying new ranks to LoRA modules...\n  - base_model.model.model.layers.0.self_attn.q_proj: r=16 -> 2 (Score: 1.0352)\n  - base_model.model.model.layers.0.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.0.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.0.self_attn.o_proj: r=16 -> 2 (Score: 0.9969)\n  - base_model.model.model.layers.1.self_attn.q_proj: r=16 -> 2 (Score: 1.0004)\n  - base_model.model.model.layers.1.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.1.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.1.self_attn.o_proj: r=16 -> 2 (Score: 1.0013)\n  - base_model.model.model.layers.2.self_attn.q_proj: r=16 -> 2 (Score: 0.9901)\n  - base_model.model.model.layers.2.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.2.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.2.self_attn.o_proj: r=16 -> 2 (Score: 1.0020)\n  - base_model.model.model.layers.3.self_attn.q_proj: r=16 -> 2 (Score: 1.0075)\n  - base_model.model.model.layers.3.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.3.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.3.self_attn.o_proj: r=16 -> 2 (Score: 0.9840)\n  - base_model.model.model.layers.4.self_attn.q_proj: r=16 -> 2 (Score: 1.0028)\n  - base_model.model.model.layers.4.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.4.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.4.self_attn.o_proj: r=16 -> 2 (Score: 0.9921)\n  - base_model.model.model.layers.5.self_attn.q_proj: r=16 -> 2 (Score: 0.9998)\n  - base_model.model.model.layers.5.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.5.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.5.self_attn.o_proj: r=16 -> 2 (Score: 1.0205)\n  - base_model.model.model.layers.6.self_attn.q_proj: r=16 -> 2 (Score: 1.0032)\n  - base_model.model.model.layers.6.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.6.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.6.self_attn.o_proj: r=16 -> 2 (Score: 0.9964)\n  - base_model.model.model.layers.7.self_attn.q_proj: r=16 -> 2 (Score: 1.0042)\n  - base_model.model.model.layers.7.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.7.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.7.self_attn.o_proj: r=16 -> 2 (Score: 1.0098)\n  - base_model.model.model.layers.8.self_attn.q_proj: r=16 -> 2 (Score: 0.9799)\n  - base_model.model.model.layers.8.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.8.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.8.self_attn.o_proj: r=16 -> 2 (Score: 1.0016)\n  - base_model.model.model.layers.9.self_attn.q_proj: r=16 -> 2 (Score: 1.0028)\n  - base_model.model.model.layers.9.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.9.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.9.self_attn.o_proj: r=16 -> 2 (Score: 0.9950)\n  - base_model.model.model.layers.10.self_attn.q_proj: r=16 -> 2 (Score: 1.0145)\n  - base_model.model.model.layers.10.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.10.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.10.self_attn.o_proj: r=16 -> 2 (Score: 0.9957)\n  - base_model.model.model.layers.11.self_attn.q_proj: r=16 -> 2 (Score: 1.0190)\n  - base_model.model.model.layers.11.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.11.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.11.self_attn.o_proj: r=16 -> 2 (Score: 0.9845)\n  - base_model.model.model.layers.12.self_attn.q_proj: r=16 -> 2 (Score: 1.0252)\n  - base_model.model.model.layers.12.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.12.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.12.self_attn.o_proj: r=16 -> 2 (Score: 0.9938)\n  - base_model.model.model.layers.13.self_attn.q_proj: r=16 -> 2 (Score: 0.9985)\n  - base_model.model.model.layers.13.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.13.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.13.self_attn.o_proj: r=16 -> 2 (Score: 0.9881)\n  - base_model.model.model.layers.14.self_attn.q_proj: r=16 -> 2 (Score: 0.9979)\n  - base_model.model.model.layers.14.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.14.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.14.self_attn.o_proj: r=16 -> 2 (Score: 1.0178)\n  - base_model.model.model.layers.15.self_attn.q_proj: r=16 -> 2 (Score: 0.9983)\n  - base_model.model.model.layers.15.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.15.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.15.self_attn.o_proj: r=16 -> 2 (Score: 1.0032)\n  - base_model.model.model.layers.16.self_attn.q_proj: r=16 -> 2 (Score: 1.0074)\n  - base_model.model.model.layers.16.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.16.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.16.self_attn.o_proj: r=16 -> 2 (Score: 1.0077)\n  - base_model.model.model.layers.17.self_attn.q_proj: r=16 -> 2 (Score: 0.9900)\n  - base_model.model.model.layers.17.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.17.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.17.self_attn.o_proj: r=16 -> 2 (Score: 1.0008)\n  - base_model.model.model.layers.18.self_attn.q_proj: r=16 -> 2 (Score: 0.9941)\n  - base_model.model.model.layers.18.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.18.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.18.self_attn.o_proj: r=16 -> 2 (Score: 1.0045)\n  - base_model.model.model.layers.19.self_attn.q_proj: r=16 -> 2 (Score: 0.9976)\n  - base_model.model.model.layers.19.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.19.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.19.self_attn.o_proj: r=16 -> 2 (Score: 1.0090)\n  - base_model.model.model.layers.20.self_attn.q_proj: r=16 -> 2 (Score: 1.0009)\n  - base_model.model.model.layers.20.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.20.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.20.self_attn.o_proj: r=16 -> 2 (Score: 1.0027)\n  - base_model.model.model.layers.21.self_attn.q_proj: r=16 -> 2 (Score: 1.0034)\n  - base_model.model.model.layers.21.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.21.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.21.self_attn.o_proj: r=16 -> 2 (Score: 1.0011)\n  - base_model.model.model.layers.22.self_attn.q_proj: r=16 -> 2 (Score: 1.0149)\n  - base_model.model.model.layers.22.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.22.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.22.self_attn.o_proj: r=16 -> 2 (Score: 1.0080)\n  - base_model.model.model.layers.23.self_attn.q_proj: r=16 -> 2 (Score: 0.9940)\n  - base_model.model.model.layers.23.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.23.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.23.self_attn.o_proj: r=16 -> 2 (Score: 0.9944)\n  - base_model.model.model.layers.24.self_attn.q_proj: r=16 -> 2 (Score: 0.9899)\n  - base_model.model.model.layers.24.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.24.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.24.self_attn.o_proj: r=16 -> 2 (Score: 1.0025)\n  - base_model.model.model.layers.25.self_attn.q_proj: r=16 -> 2 (Score: 1.0117)\n  - base_model.model.model.layers.25.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.25.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.25.self_attn.o_proj: r=16 -> 2 (Score: 0.9917)\n  - base_model.model.model.layers.26.self_attn.q_proj: r=16 -> 2 (Score: 1.0056)\n  - base_model.model.model.layers.26.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.26.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.26.self_attn.o_proj: r=16 -> 2 (Score: 0.9943)\n  - base_model.model.model.layers.27.self_attn.q_proj: r=16 -> 2 (Score: 0.9914)\n  - base_model.model.model.layers.27.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.27.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.27.self_attn.o_proj: r=16 -> 2 (Score: 1.0122)\n  - base_model.model.model.layers.28.self_attn.q_proj: r=16 -> 2 (Score: 1.0136)\n  - base_model.model.model.layers.28.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.28.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.28.self_attn.o_proj: r=16 -> 2 (Score: 0.9984)\n  - base_model.model.model.layers.29.self_attn.q_proj: r=16 -> 2 (Score: 0.9977)\n  - base_model.model.model.layers.29.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.29.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.29.self_attn.o_proj: r=16 -> 2 (Score: 0.9863)\n  - base_model.model.model.layers.30.self_attn.q_proj: r=16 -> 2 (Score: 0.9974)\n  - base_model.model.model.layers.30.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.30.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.30.self_attn.o_proj: r=16 -> 2 (Score: 0.9958)\n  - base_model.model.model.layers.31.self_attn.q_proj: r=16 -> 2 (Score: 1.0128)\n  - base_model.model.model.layers.31.self_attn.k_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.31.self_attn.v_proj: r=16 -> 0 (Score: 0.0000)\n  - base_model.model.model.layers.31.self_attn.o_proj: r=16 -> 2 (Score: 0.9931)\n--- AdaptiveLoRA: Update complete. Logs saved to ./logs/adaptive_lora_epoch_logs.csv ---\n\n--- AdaptiveLoRA: Starting rank update for Epoch 2 ---\nComputing BI importance scores...\nAllocating new ranks...\nApplying new ranks to LoRA modules...\n  - base_model.model.model.layers.0.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.0.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.0.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.0.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.1.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.1.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.1.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.1.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.2.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.2.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.2.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.2.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.3.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.3.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.3.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.3.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.4.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.4.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.4.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.4.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.5.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.5.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.5.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.5.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.6.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.6.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.6.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.6.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.7.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.7.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.7.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.7.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.8.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.8.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.8.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.8.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.9.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.9.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.9.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.9.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.10.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.10.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.10.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.10.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.11.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.11.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.11.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.11.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.12.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.12.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.12.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.12.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.13.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.13.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.13.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.13.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.14.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.14.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.14.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.14.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.15.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.15.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.15.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.15.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.16.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.16.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.16.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.16.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.17.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.17.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.17.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.17.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.18.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.18.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.18.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.18.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.19.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.19.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.19.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.19.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.20.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.20.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.20.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.20.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.21.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.21.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.21.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.21.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.22.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.22.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.22.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.22.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.23.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.23.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.23.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.23.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.24.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.24.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.24.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.24.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.25.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.25.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.25.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.25.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.26.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.26.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.26.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.26.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.27.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.27.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.27.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.27.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.28.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.28.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.28.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.28.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.29.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.29.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.29.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.29.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.30.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.30.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.30.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.30.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.31.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.31.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.31.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.31.self_attn.o_proj: r=2 (Unchanged)\n--- AdaptiveLoRA: Update complete. Logs saved to ./logs/adaptive_lora_epoch_logs.csv ---\n\n--- AdaptiveLoRA: Starting rank update for Epoch 3 ---\nComputing BI importance scores...\nAllocating new ranks...\nApplying new ranks to LoRA modules...\n  - base_model.model.model.layers.0.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.0.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.0.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.0.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.1.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.1.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.1.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.1.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.2.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.2.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.2.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.2.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.3.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.3.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.3.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.3.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.4.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.4.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.4.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.4.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.5.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.5.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.5.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.5.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.6.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.6.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.6.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.6.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.7.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.7.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.7.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.7.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.8.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.8.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.8.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.8.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.9.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.9.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.9.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.9.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.10.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.10.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.10.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.10.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.11.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.11.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.11.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.11.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.12.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.12.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.12.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.12.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.13.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.13.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.13.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.13.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.14.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.14.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.14.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.14.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.15.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.15.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.15.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.15.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.16.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.16.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.16.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.16.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.17.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.17.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.17.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.17.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.18.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.18.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.18.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.18.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.19.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.19.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.19.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.19.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.20.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.20.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.20.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.20.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.21.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.21.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.21.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.21.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.22.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.22.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.22.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.22.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.23.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.23.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.23.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.23.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.24.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.24.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.24.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.24.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.25.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.25.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.25.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.25.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.26.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.26.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.26.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.26.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.27.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.27.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.27.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.27.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.28.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.28.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.28.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.28.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.29.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.29.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.29.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.29.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.30.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.30.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.30.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.30.self_attn.o_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.31.self_attn.q_proj: r=2 (Unchanged)\n  - base_model.model.model.layers.31.self_attn.k_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.31.self_attn.v_proj: r=0 (Unchanged)\n  - base_model.model.model.layers.31.self_attn.o_proj: r=2 (Unchanged)\n--- AdaptiveLoRA: Update complete. Logs saved to ./logs/adaptive_lora_epoch_logs.csv ---\nTraining complete. Check logs/adaptive_lora_epoch_logs.csv for rank evolution.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}