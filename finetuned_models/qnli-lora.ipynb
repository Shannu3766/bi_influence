{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install evaluate\n",
    "!pip install -U datasets==2.20.0 pyarrow==15.0.2 transformers==4.44.2 evaluate==0.4.2 --no-cache-dir\n",
    "!pip install -q datasets evaluate accelerate scikit-learn pandas matplotlib\n",
    "!pip install -U \"transformers>=4.41\" accelerate safetensors\n",
    "!pip install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c4838",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --no-cache-dir git+https://github.com/Shannu3766/bi_influence.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cea382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import evaluate\n",
    "from adaptive_lora.callbacks import AdaptiveLoRACallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17a4966",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \n",
    "output_dir = \"./tinyllama-qnli-lora\"\n",
    "seed = 42\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af41dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "eval_batch_size = 32\n",
    "num_train_epochs = 3\n",
    "max_length = 128\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "rank=8\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=rank,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[   \n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be3753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(s: Optional[str]) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    return \" \".join(str(s).strip().split())\n",
    "\n",
    "def build_one_shot_demo(example: dict) -> str:\n",
    "    q = clean_text(example[\"question\"])\n",
    "    c = clean_text(example[\"context\"])\n",
    "    lbl = example.get(\"label_text\", \"Yes\")\n",
    "    return f\"Example:\\nQuestion: {q}\\nContext: {c}\\nAnswer (Yes/No): {lbl}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d75325",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_SHOT_EXAMPLE = {\n",
    "    \"question\": \"Who wrote Hamlet?\",\n",
    "    \"context\": \"Hamlet was written by William Shakespeare and first performed in the early 17th century.\",\n",
    "    \"label_text\": \"Yes\",  # \"Yes\" => the context contains the answer\n",
    "}\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"qnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2cf486",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    # safe default: use eos_token as pad\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fda3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load tokenizer\n",
    "\n",
    "\n",
    "def preprocess_function(examples, *, max_length: int = max_length, add_demo: bool = False):\n",
    "    \"\"\"\n",
    "    Build instruction-like prompts and tokenize them.\n",
    "    - add_demo: if True, prepends a 1-shot example to each prompt (can help decoder-only models)\n",
    "    \"\"\"\n",
    "    questions = [clean_text(q) for q in examples[\"question\"]]\n",
    "    contexts = [clean_text(s) for s in examples[\"sentence\"]]\n",
    "\n",
    "    demo_str = build_one_shot_demo(ONE_SHOT_EXAMPLE) if add_demo else \"\"\n",
    "\n",
    "    prompts = []\n",
    "    for q, c in zip(questions, contexts):\n",
    "        # Instruction-style prompt ending with a short, constrained label target\n",
    "        prompt = (\n",
    "            \"You are a helpful assistant.\\n\"\n",
    "            f\"{demo_str}\"\n",
    "            f\"Question: {q}\\n\"\n",
    "            f\"Context: {c}\\n\"\n",
    "            \"Answer (Yes/No):\"\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        prompts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False, \n",
    "    )\n",
    "\n",
    "    if \"label\" in examples:\n",
    "        tokenized[\"labels\"] = examples[\"label\"]\n",
    "    elif \"labels\" in examples:\n",
    "        tokenized[\"labels\"] = examples[\"labels\"]\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized = dataset.map(\n",
    "    lambda ex: preprocess_function(ex, max_length=max_length, add_demo=False),\n",
    "    batched=True,\n",
    "    remove_columns=[\"question\", \"sentence\", \"idx\"],\n",
    ")\n",
    "\n",
    "if \"label\" in tokenized[\"train\"].column_names and \"labels\" not in tokenized[\"train\"].column_names:\n",
    "    tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
    "\n",
    "tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\", pad_to_multiple_of=8)\n",
    "train_dataset = tokenized[\"train\"].select(range(10000))\n",
    "eval_dataset = tokenized[\"validation\"].select(range(1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594945b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=2,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ccc394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Resize token embeddings if tokenizer changed\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "\n",
    "# -------------------------\n",
    "# Apply LoRA (PEFT)\n",
    "# -------------------------\n",
    "print(\"Applying LoRA (PEFT)...\")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30496363",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=eval_batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65475fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    acc = accuracy.compute(predictions=preds, references=labels)\n",
    "    return {\"accuracy\": acc[\"accuracy\"]}\n",
    "\n",
    "# -------------------------\n",
    "# TrainingArguments + Trainer\n",
    "# -------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    seed=seed,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Saving model and adapters...\")\n",
    "trainer.save_model(output_dir)\n",
    "print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5634b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
