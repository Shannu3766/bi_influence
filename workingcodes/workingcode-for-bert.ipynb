{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:20:41.519606Z","iopub.execute_input":"2025-10-20T18:20:41.519955Z","iopub.status.idle":"2025-10-20T18:20:41.794969Z","shell.execute_reply.started":"2025-10-20T18:20:41.519890Z","shell.execute_reply":"2025-10-20T18:20:41.794115Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install git+https://github.com/Shannu3766/bi_influence.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:20:41.796069Z","iopub.execute_input":"2025-10-20T18:20:41.796340Z","iopub.status.idle":"2025-10-20T18:22:11.846067Z","shell.execute_reply.started":"2025-10-20T18:20:41.796323Z","shell.execute_reply":"2025-10-20T18:22:11.845111Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/Shannu3766/bi_influence.git\n  Cloning https://github.com/Shannu3766/bi_influence.git to /tmp/pip-req-build-t3py0trb\n  Running command git clone --filter=blob:none --quiet https://github.com/Shannu3766/bi_influence.git /tmp/pip-req-build-t3py0trb\n  Resolved https://github.com/Shannu3766/bi_influence.git to commit e768abe24613868eff413a83dff718b6df358305\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from adalora_bi==0.1.0) (2.6.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from adalora_bi==0.1.0) (4.53.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from adalora_bi==0.1.0) (4.1.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from adalora_bi==0.1.0) (1.26.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->adalora_bi==0.1.0) (3.19.1)\nCollecting pyarrow>=21.0.0 (from datasets->adalora_bi==0.1.0)\n  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->adalora_bi==0.1.0) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->adalora_bi==0.1.0) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->adalora_bi==0.1.0) (2.32.5)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets->adalora_bi==0.1.0) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->adalora_bi==0.1.0) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->adalora_bi==0.1.0) (0.70.16)\nRequirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->adalora_bi==0.1.0) (2025.9.0)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->adalora_bi==0.1.0) (1.0.0rc2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->adalora_bi==0.1.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->adalora_bi==0.1.0) (6.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->adalora_bi==0.1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->adalora_bi==0.1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->adalora_bi==0.1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->adalora_bi==0.1.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->adalora_bi==0.1.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->adalora_bi==0.1.0) (2.4.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->adalora_bi==0.1.0) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->adalora_bi==0.1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->adalora_bi==0.1.0) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->adalora_bi==0.1.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->adalora_bi==0.1.0)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->adalora_bi==0.1.0)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->adalora_bi==0.1.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->adalora_bi==0.1.0)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->adalora_bi==0.1.0)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->adalora_bi==0.1.0)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->adalora_bi==0.1.0)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->adalora_bi==0.1.0)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->adalora_bi==0.1.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->adalora_bi==0.1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->adalora_bi==0.1.0) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->adalora_bi==0.1.0)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->adalora_bi==0.1.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->adalora_bi==0.1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->adalora_bi==0.1.0) (1.3.0)\nCollecting huggingface-hub>=0.24.0 (from datasets->adalora_bi==0.1.0)\n  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->adalora_bi==0.1.0) (2025.9.18)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->adalora_bi==0.1.0) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->adalora_bi==0.1.0) (0.5.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->adalora_bi==0.1.0) (3.12.15)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets->adalora_bi==0.1.0) (1.1.10)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->adalora_bi==0.1.0) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->adalora_bi==0.1.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->adalora_bi==0.1.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->adalora_bi==0.1.0) (2025.8.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->adalora_bi==0.1.0) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->adalora_bi==0.1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->adalora_bi==0.1.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->adalora_bi==0.1.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->adalora_bi==0.1.0) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->adalora_bi==0.1.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->adalora_bi==0.1.0) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->adalora_bi==0.1.0) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->adalora_bi==0.1.0) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->adalora_bi==0.1.0) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->adalora_bi==0.1.0) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->adalora_bi==0.1.0) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->adalora_bi==0.1.0) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->adalora_bi==0.1.0) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->adalora_bi==0.1.0) (1.20.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->adalora_bi==0.1.0) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->adalora_bi==0.1.0) (1.17.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: adalora_bi\n  Building wheel for adalora_bi (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for adalora_bi: filename=adalora_bi-0.1.0-py3-none-any.whl size=7141 sha256=9ff347db785812110d42bc5367e391ede501b71e084eb114f77db1012a8d59eb\n  Stored in directory: /tmp/pip-ephem-wheel-cache-5n6qpori/wheels/4b/91/1d/e153ff3aa3759d6ccdcc4706d8559f1cbfaa4aa22b992958cd\nSuccessfully built adalora_bi\nInstalling collected packages: pyarrow, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, nvidia-cusolver-cu12, adalora_bi\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed adalora_bi-0.1.0 huggingface-hub-0.35.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyarrow-21.0.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\"\"\"\nExample: run dynamic BI-based AdaLoRA fine-tuning on a tiny dummy dataset.\n\nThis demo:\n - builds small dataset using HuggingFace datasets or dummy data,\n - computes BI scores each epoch,\n - reallocates LoRA ranks each epoch,\n - trains adapters for that epoch.\n\nRun: python adalora_bi/examples/finetune_bi_demo.py\n\"\"\"\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom adalora_bi import fine_tune_lora_dynamic\n\n# Simple dummy dataset for demo\nclass DummyTextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=64):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        enc = self.tokenizer(self.texts[idx],\n                             truncation=True,\n                             padding=\"max_length\",\n                             max_length=self.max_length,\n                             return_tensors=\"pt\")\n        item = {k: v.squeeze(0) for k, v in enc.items()}\n        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item\n\ndef collate_fn(batch):\n    # batch already dict tensors\n    out = {}\n    for k in batch[0].keys():\n        out[k] = torch.stack([b[k] for b in batch], dim=0)\n    return out\n\ndef main():\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(device)\n    model_name = \"distilbert-base-uncased\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n    # Dummy texts\n    texts = [\n        \"I love programming in Python\",\n        \"The weather is sunny today\",\n        \"Transformers are great\",\n        \"I had pizza\",\n        \"PyTorch is awesome\",\n        \"OpenAI builds models\",\n        \"I went for a run\",\n        \"I enjoy reading books\"\n    ]\n    labels = [0,1,0,1,0,0,1,1]\n\n    ds = DummyTextDataset(texts, labels, tokenizer)\n    train_loader = DataLoader(ds, batch_size=2, shuffle=True, collate_fn=collate_fn)\n    val_loader = DataLoader(ds, batch_size=2, shuffle=False, collate_fn=collate_fn)\n\n    # Run dynamic training: BI recomputed at start of each epoch (so 2 epochs -> computed twice)\n    fine_tune_lora_dynamic(\n    model,\n    train_loader,\n    val_loader=val_loader,\n    device=device,\n    total_R=48,\n    tau=0.5,\n    epochs=4,\n    lr=5e-4,\n    max_batches_for_bi=2,     # only 2 batches for BI scoring\n    recompute_every=2,        # recompute BI every 2 epochs\n    fast_mode=False,           # monitors fewer modules\n)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:22:11.847164Z","iopub.execute_input":"2025-10-20T18:22:11.847426Z","iopub.status.idle":"2025-10-20T18:22:40.316484Z","shell.execute_reply.started":"2025-10-20T18:22:11.847403Z","shell.execute_reply":"2025-10-20T18:22:40.315633Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36619650a73542df999d1b9bcb44f9bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af85b1b3e9714f24abeb74638586fbbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7893bcc701f4a1a8cc9d2438c11de16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3956da2d6034dfa8578fa96598d0094"}},"metadata":{}},{"name":"stderr","text":"2025-10-20 18:22:25.400015: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760984545.557250      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760984545.611380      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fc05fabdcb84dc2b65d419dbbea7ae8"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n=== Epoch 1/4 ===\nComputing BI importance...\nBI scores collected for 20 modules in 0.44s\nAllocated ranks (first few):\n  distilbert.transformer.layer.0.attention.q_lin -> r=3\n  distilbert.transformer.layer.0.attention.k_lin -> r=3\n  distilbert.transformer.layer.0.attention.v_lin -> r=2\n  distilbert.transformer.layer.0.attention.out_lin -> r=7\n  distilbert.transformer.layer.0.ffn.lin1 -> r=1\n  distilbert.transformer.layer.0.ffn.lin2 -> r=2\n  distilbert.transformer.layer.1.attention.q_lin -> r=1\n  distilbert.transformer.layer.1.attention.k_lin -> r=1\n  distilbert.transformer.layer.1.attention.v_lin -> r=6\n  distilbert.transformer.layer.1.attention.out_lin -> r=1\nPatched 20 modules with LoRA adapters.\n","output_type":"stream"},{"name":"stderr","text":"Training epoch 1: 100%|██████████| 4/4 [00:00<00:00, 11.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 step 4 avg loss 0.2769\nAfter epoch 1: val loss=0.6868, acc=0.5000\n\n=== Epoch 2/4 ===\n","output_type":"stream"},{"name":"stderr","text":"Training epoch 2: 100%|██████████| 4/4 [00:00<00:00, 54.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 step 4 avg loss 0.2708\nAfter epoch 2: val loss=0.6752, acc=0.6250\n\n=== Epoch 3/4 ===\nComputing BI importance...\nBI scores collected for 20 modules in 0.06s\nAllocated ranks (first few):\n  distilbert.transformer.layer.0.attention.q_lin.orig -> r=3\n  distilbert.transformer.layer.0.attention.k_lin.orig -> r=3\n  distilbert.transformer.layer.0.attention.v_lin.orig -> r=2\n  distilbert.transformer.layer.0.attention.out_lin.orig -> r=7\n  distilbert.transformer.layer.0.ffn.lin1.orig -> r=1\n  distilbert.transformer.layer.0.ffn.lin2.orig -> r=2\n  distilbert.transformer.layer.1.attention.q_lin.orig -> r=1\n  distilbert.transformer.layer.1.attention.k_lin.orig -> r=2\n  distilbert.transformer.layer.1.attention.v_lin.orig -> r=6\n  distilbert.transformer.layer.1.attention.out_lin.orig -> r=1\nPatched 20 modules with LoRA adapters.\n","output_type":"stream"},{"name":"stderr","text":"Training epoch 3: 100%|██████████| 4/4 [00:00<00:00, 37.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 step 4 avg loss 0.2811\nAfter epoch 3: val loss=0.6521, acc=1.0000\n\n=== Epoch 4/4 ===\n","output_type":"stream"},{"name":"stderr","text":"Training epoch 4: 100%|██████████| 4/4 [00:00<00:00, 29.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 step 4 avg loss 0.2531\nAfter epoch 4: val loss=0.6044, acc=1.0000\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -q --upgrade pyarrow datasets pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:22:40.318667Z","iopub.execute_input":"2025-10-20T18:22:40.319213Z","iopub.status.idle":"2025-10-20T18:22:52.229839Z","shell.execute_reply.started":"2025-10-20T18:22:40.319192Z","shell.execute_reply":"2025-10-20T18:22:52.228870Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m96.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.3/506.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install --upgrade --force-reinstall \"pyarrow>=14.0.1,<17\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:22:52.231074Z","iopub.execute_input":"2025-10-20T18:22:52.231391Z","iopub.status.idle":"2025-10-20T18:23:01.962622Z","shell.execute_reply.started":"2025-10-20T18:22:52.231358Z","shell.execute_reply":"2025-10-20T18:23:01.961583Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting pyarrow<17,>=14.0.1\n  Downloading pyarrow-16.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\nCollecting numpy>=1.16.6 (from pyarrow<17,>=14.0.1)\n  Downloading numpy-2.3.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-16.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading numpy-2.3.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, pyarrow\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 21.0.0\n    Uninstalling pyarrow-21.0.0:\n      Successfully uninstalled pyarrow-21.0.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.2.0 requires pyarrow>=21.0.0, but you have pyarrow 16.1.0 which is incompatible.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.4 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.4 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.4 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.4 which is incompatible.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-2.3.4 pyarrow-16.1.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install --upgrade --force-reinstall \"numpy<2.0\" \"scipy>=1.10,<1.14\" \"scikit-learn>=1.3,<1.5\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:23:01.963811Z","iopub.execute_input":"2025-10-20T18:23:01.964149Z","iopub.status.idle":"2025-10-20T18:23:28.014320Z","shell.execute_reply.started":"2025-10-20T18:23:01.964109Z","shell.execute_reply":"2025-10-20T18:23:28.013227Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting numpy<2.0\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting scipy<1.14,>=1.10\n  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m974.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting scikit-learn<1.5,>=1.3\n  Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting joblib>=1.2.0 (from scikit-learn<1.5,>=1.3)\n  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\nCollecting threadpoolctl>=2.0.0 (from scikit-learn<1.5,>=1.3)\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\nDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.4/308.4 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nInstalling collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n  Attempting uninstall: threadpoolctl\n    Found existing installation: threadpoolctl 3.6.0\n    Uninstalling threadpoolctl-3.6.0:\n      Successfully uninstalled threadpoolctl-3.6.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.3.4\n    Uninstalling numpy-2.3.4:\n      Successfully uninstalled numpy-2.3.4\n  Attempting uninstall: joblib\n    Found existing installation: joblib 1.5.2\n    Uninstalling joblib-1.5.2:\n      Successfully uninstalled joblib-1.5.2\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.15.3\n    Uninstalling scipy-1.15.3:\n      Successfully uninstalled scipy-1.15.3\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.2.0 requires pyarrow>=21.0.0, but you have pyarrow 16.1.0 which is incompatible.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed joblib-1.5.2 numpy-1.26.4 scikit-learn-1.4.2 scipy-1.13.1 threadpoolctl-3.6.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\"\"\"\nFine-tune DistilBERT on SST-2 using BI-based Adaptive LoRA.\nThis demonstrates a full run on a public dataset with saving.\n\nRun:\n    python -m adalora_bi.examples.finetune_sst2_bi\n\"\"\"\n\nimport os\nimport torch\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, default_data_collator\nfrom adalora_bi import fine_tune_lora_dynamic\n\n\ndef main():\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n\n    # ------------------------------------------------------------------\n    # 1️⃣  Load public dataset (GLUE SST-2)\n    # ------------------------------------------------------------------\n    dataset = load_dataset(\"glue\", \"sst2\")\n    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n    def tokenize_fn(batch):\n        return tokenizer(\n            batch[\"sentence\"],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=128,\n        )\n\n    tokenized = dataset.map(tokenize_fn, batched=True)\n    tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\n    train_ds = tokenized[\"train\"].shuffle(seed=42).select(range(2000))  # small subset for demo\n    val_ds = tokenized[\"validation\"].select(range(500))\n\n    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=default_data_collator)\n    val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, collate_fn=default_data_collator)\n\n    # ------------------------------------------------------------------\n    # 2️⃣  Load model\n    # ------------------------------------------------------------------\n    model = AutoModelForSequenceClassification.from_pretrained(\n        \"distilbert-base-uncased\",\n        num_labels=2\n    )\n\n    # ------------------------------------------------------------------\n    # 3️⃣  Fine-tune with BI-based adaptive LoRA\n    # ------------------------------------------------------------------\n    fine_tune_lora_dynamic(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        device=device,\n        total_R=32,          # total LoRA rank budget\n        tau=0.5,             # temperature\n        epochs=2,            # recompute BI every epoch\n        lr=2e-5,\n        weight_decay=0.01,\n        max_batches_for_bi=8,\n        recompute_every=1,\n        fast_mode=False,     # collect all Linear layers for real benchmark\n    )\n\n    # ------------------------------------------------------------------\n    # 4️⃣  Save the fine-tuned model\n    # ------------------------------------------------------------------\n    save_dir = \"./saved_models\"\n    os.makedirs(save_dir, exist_ok=True)\n    save_path = os.path.join(save_dir, \"adalora_bi_sst2.pt\")\n\n    torch.save(model.state_dict(), save_path)\n    print(f\"\\n✅ Model saved successfully to: {save_path}\\n\")\n\n    # Optional: evaluate final model\n    model.eval()\n    print(\"Final evaluation on validation set...\")\n    total, correct = 0, 0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            preds = outputs.logits.argmax(dim=-1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            total += batch[\"labels\"].size(0)\n    print(f\"Validation Accuracy: {correct/total:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:23:28.015680Z","iopub.execute_input":"2025-10-20T18:23:28.016146Z","iopub.status.idle":"2025-10-20T18:24:18.844846Z","shell.execute_reply.started":"2025-10-20T18:23:28.016109Z","shell.execute_reply":"2025-10-20T18:24:18.843918Z"}},"outputs":[{"name":"stderr","text":"<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.ChunkedArray size changed, may indicate binary incompatibility. Expected 64 from C header, got 72 from PyObject\n<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib._Tabular size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject\n<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.Table size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"719258a1f7c04ada8a3dc8a7f0afa4f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sst2/train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf9e81e4718d49d4a939afb3d4dbf7e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sst2/validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d04e34158a241b1b232fc9fa9a5bfcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sst2/test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5ab80360f5d4105868638c8003bfc6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc3128f264ea4937a3a20007e403c4ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32726858561043c9a98cb4e9280a5708"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2926086b9a3c4f84be4f616588057d6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1dcc266399345159beff702c9c33975"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3fc10e5f1324fd398b934c74e0e5a0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ebbe623b2de49a3b7ff6c42002c4728"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n=== Epoch 1/2 ===\nComputing BI importance...\nBI scores collected for 20 modules in 1.57s\nAllocated ranks (first few):\n  distilbert.transformer.layer.0.attention.q_lin -> r=1\n  distilbert.transformer.layer.0.attention.k_lin -> r=1\n  distilbert.transformer.layer.0.attention.v_lin -> r=1\n  distilbert.transformer.layer.0.attention.out_lin -> r=5\n  distilbert.transformer.layer.0.ffn.lin1 -> r=2\n  distilbert.transformer.layer.0.ffn.lin2 -> r=1\n  distilbert.transformer.layer.1.attention.q_lin -> r=2\n  distilbert.transformer.layer.1.attention.k_lin -> r=1\n  distilbert.transformer.layer.1.attention.v_lin -> r=4\n  distilbert.transformer.layer.1.attention.out_lin -> r=2\nPatched 20 modules with LoRA adapters.\n","output_type":"stream"},{"name":"stderr","text":"Training epoch 1: 100%|██████████| 250/250 [00:14<00:00, 17.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 step 250 avg loss 17.6416\nAfter epoch 1: val loss=0.6920, acc=0.5000\n\n=== Epoch 2/2 ===\nComputing BI importance...\nBI scores collected for 20 modules in 1.38s\nAllocated ranks (first few):\n  distilbert.transformer.layer.0.attention.q_lin.orig -> r=1\n  distilbert.transformer.layer.0.attention.k_lin.orig -> r=1\n  distilbert.transformer.layer.0.attention.v_lin.orig -> r=1\n  distilbert.transformer.layer.0.attention.out_lin.orig -> r=4\n  distilbert.transformer.layer.0.ffn.lin1.orig -> r=2\n  distilbert.transformer.layer.0.ffn.lin2.orig -> r=1\n  distilbert.transformer.layer.1.attention.q_lin.orig -> r=2\n  distilbert.transformer.layer.1.attention.k_lin.orig -> r=1\n  distilbert.transformer.layer.1.attention.v_lin.orig -> r=4\n  distilbert.transformer.layer.1.attention.out_lin.orig -> r=1\nPatched 20 modules with LoRA adapters.\n","output_type":"stream"},{"name":"stderr","text":"Training epoch 2: 100%|██████████| 250/250 [00:16<00:00, 15.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 step 250 avg loss 17.0408\nAfter epoch 2: val loss=0.6787, acc=0.5700\n\n✅ Model saved successfully to: ./saved_models/adalora_bi_sst2.pt\n\nFinal evaluation on validation set...\nValidation Accuracy: 0.5700\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pyarrow\nprint(pyarrow.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:24:18.846035Z","iopub.execute_input":"2025-10-20T18:24:18.846375Z","iopub.status.idle":"2025-10-20T18:24:18.851008Z","shell.execute_reply.started":"2025-10-20T18:24:18.846343Z","shell.execute_reply":"2025-10-20T18:24:18.850146Z"}},"outputs":[{"name":"stdout","text":"19.0.1\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}